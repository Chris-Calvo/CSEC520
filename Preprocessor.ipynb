{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import whois\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import socket\n",
    "import dns.resolver\n",
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ssl_state(url):\n",
    "    if \"https://\" in url:\n",
    "        try:\n",
    "            response = requests.head(url, verify=True)\n",
    "            return 1\n",
    "        except requests.exceptions.SSLError:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_domain_registration_length(url, days_threshold=365):\n",
    "    try:\n",
    "        domain_info = whois.whois(url)\n",
    "        expiration_date = domain_info.expiration_date\n",
    "        if isinstance(expiration_date, list):\n",
    "            expiration_date = expiration_date[0]\n",
    "        if expiration_date is not None and (expiration_date - datetime.now()).days <= days_threshold:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    except whois.parser.PywhoisError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_favicon(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        favicon_link = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')\n",
    "\n",
    "        if favicon_link:\n",
    "            favicon_href = favicon_link.get('href')\n",
    "            absolute_favicon_url = urljoin(url, favicon_href)\n",
    "            return -1 if urlparse(url).netloc != urlparse(absolute_favicon_url).netloc else 1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ports(url):\n",
    "    try:\n",
    "        hostname = urlparse(url).hostname\n",
    "        open_ports = [80, 443]\n",
    "        closed_ports = [21, 22, 23, 445, 1433, 1521, 3306, 3389]\n",
    "        open_ports_status = all(is_port_open(hostname, port) for port in open_ports)\n",
    "        closed_ports_status = all(not is_port_open(hostname, port) for port in closed_ports)\n",
    "        return 1 if open_ports_status and closed_ports_status else -1\n",
    "\n",
    "    except socket.gaierror as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return -1\n",
    "\n",
    "def is_port_open(hostname, port):\n",
    "    try:\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(1)  \n",
    "        server_address = (hostname, port)\n",
    "        result = sock.connect_ex(server_address)\n",
    "        sock.close()\n",
    "        return result == 0\n",
    "\n",
    "    except socket.error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_request_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        total_requests = 0\n",
    "        external_requests = 0\n",
    "\n",
    "        for tag in soup.find_all(['img', 'video', 'audio']):\n",
    "            src = tag.get('src')\n",
    "            if src:\n",
    "                total_requests += 1\n",
    "                absolute_url = urljoin(url, src)\n",
    "                if urlparse(absolute_url).netloc != urlparse(url).netloc:\n",
    "                    external_requests += 1\n",
    "\n",
    "        if total_requests == 0:\n",
    "            return -1  \n",
    "\n",
    "        percentage = (external_requests / total_requests) * 100\n",
    "\n",
    "        if percentage < 22:\n",
    "            return 1\n",
    "        elif 22 <= percentage <= 61:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_url_of_anchor(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        total_anchors = 0\n",
    "        external_anchors = 0\n",
    "        no_webpage_anchors = 0\n",
    "\n",
    "        for tag in soup.find_all('a'):\n",
    "            href = tag.get('href')\n",
    "            if href:\n",
    "                total_anchors += 1\n",
    "                absolute_url = urljoin(url, href)\n",
    "                parsed_url = urlparse(absolute_url)\n",
    "\n",
    "                if parsed_url.netloc != urlparse(url).netloc:\n",
    "                    external_anchors += 1\n",
    "\n",
    "                if not parsed_url.path:\n",
    "                    no_webpage_anchors += 1\n",
    "\n",
    "        if total_anchors == 0:\n",
    "            return -1  \n",
    "\n",
    "        percentage = (external_anchors + no_webpage_anchors) / total_anchors * 100\n",
    "\n",
    "        if percentage < 31:\n",
    "            return 1\n",
    "        elif 31 <= percentage <= 67:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_links_in_tags(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return -1\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    meta_tags = soup.find_all(['meta', 'script', 'link'])\n",
    "    total_links = 0\n",
    "    links_within_tags = 0\n",
    "\n",
    "    for tag in meta_tags:\n",
    "        tag_content = str(tag)\n",
    "        links_within_tags += tag_content.count(\"href=\")  \n",
    "\n",
    "    percentage_links_within_tags = (links_within_tags / total_links) * 100 if total_links != 0 else 0\n",
    "\n",
    "    if percentage_links_within_tags < 17:\n",
    "        return 1\n",
    "    elif 17 <= percentage_links_within_tags <= 81:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sfh(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return -1\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    form_tag = soup.find('form')\n",
    "\n",
    "    if form_tag:\n",
    "        action_attribute = form_tag.get('action', '')\n",
    "\n",
    "        if not action_attribute or action_attribute == \"about:blank\":\n",
    "            return -1\n",
    "        else:\n",
    "            parsed_action_url = urlparse(action_attribute)\n",
    "            parsed_url = urlparse(url)\n",
    "            if parsed_action_url.hostname and parsed_action_url.hostname != parsed_url.hostname:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mail(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return -1\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    mail_functions = soup.find_all(string=lambda text: \"mail()\" in text)\n",
    "    mailto_links = soup.find_all(href=lambda href: href and \"mailto:\" in href)\n",
    "\n",
    "    if mail_functions or mailto_links:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_abnormal_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    if parsed_url.hostname and parsed_url.hostname in url:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_redirect(url):\n",
    "    try:\n",
    "        response = requests.get(url, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        redirects_count = len(response.history)\n",
    "\n",
    "        if redirects_count <= 1:\n",
    "            return 1\n",
    "        elif 2 <= redirects_count < 4:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error checking redirects: {e}\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_onMouseOver(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return -1\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    onmouseover_elements = soup.find_all(onmouseover=True)\n",
    "\n",
    "    for element in onmouseover_elements:\n",
    "        onmouseover_code = element.get('onMouseOver', '')\n",
    "        if 'window.status' in onmouseover_code:\n",
    "            return -1\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rightClick(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return -1\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    script_tags = soup.find_all('script')\n",
    "\n",
    "    for script_tag in script_tags:\n",
    "        script_code = script_tag.string\n",
    "        if script_code and \"event.button==2\" in script_code and \"return false\" in script_code:\n",
    "            return -1\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_popup(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return -1\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    script_tags = soup.find_all('script')\n",
    "\n",
    "    for script_tag in script_tags:\n",
    "        script_code = script_tag.string\n",
    "        if script_code and \"window.open\" in script_code and \"document.createElement('input')\" in script_code:\n",
    "            return -1\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iframe(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return -1\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    iframe_tags = soup.find_all('iframe')\n",
    "\n",
    "    if iframe_tags:\n",
    "        return -1  \n",
    "    else:\n",
    "        return 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_age_of_domain(url):\n",
    "    try:\n",
    "        domain = url.split('//')[-1].split('/')[0]\n",
    "        domain_info = whois.whois(domain)\n",
    "        creation_date = domain_info.creation_date\n",
    "\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "\n",
    "        if creation_date:\n",
    "            today = datetime.now()\n",
    "            age_in_months = (today - creation_date).days // 30\n",
    "\n",
    "            if age_in_months >= 6:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return -1\n",
    "    except whois.parser.PywhoisError as e:\n",
    "        print(f\"Error performing WHOIS lookup: {e}\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dns_record(url):\n",
    "    try:\n",
    "        domain = url.split('//')[-1].split('/')[0]\n",
    "        answers = dns.resolver.resolve(domain, 'A')\n",
    "\n",
    "        if not answers or not answers.rrset.items:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    except dns.resolver.NXDOMAIN:\n",
    "        return -1\n",
    "    except dns.resolver.Timeout:\n",
    "        print(\"DNS query timed out\")\n",
    "        return -1\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking DNS records: {e}\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_google_index(url):\n",
    "    try:\n",
    "        results = list(search(f\"site:{url}\"))\n",
    "        \n",
    "        for result in results:\n",
    "            if url in result:\n",
    "                return 1  \n",
    "        return -1 \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing Google search: {e}\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_external_links_count(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        base_url = requests.utils.urlparse(url).netloc\n",
    "        external_links_count = sum(1 for link in all_links if not link['href'].startswith(base_url))\n",
    "\n",
    "        return external_links_count\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return -1\n",
    "\n",
    "def check_links_to_webpage(url):\n",
    "    external_links_count = get_external_links_count(url)\n",
    "\n",
    "    if external_links_count == -1:\n",
    "        return -1  \n",
    "    elif 0 < external_links_count <= 2:\n",
    "        return 0  \n",
    "    else:\n",
    "        return 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_url_to_top_lists(url, top_domains, top_ips):\n",
    "    try:\n",
    "        url_domain = requests.utils.urlparse(url).netloc\n",
    "        url_ip = socket.gethostbyname(url_domain)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error parsing URL: {e}\")\n",
    "        return None\n",
    "\n",
    "    is_domain_in_top_list = url_domain in top_domains\n",
    "    is_ip_in_top_list = url_ip in top_ips\n",
    "\n",
    "    return -1 if is_domain_in_top_list or is_ip_in_top_list else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(url):\n",
    "    # Parse the URL\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Initialize the dictionary to store attribute values\n",
    "    attributes = {}\n",
    "\n",
    "    # 1. having_IP_Address\n",
    "    attributes['Having_IP_Address'] = -1 if parsed_url.hostname.replace('.', '').isdigit() else 1\n",
    "\n",
    "    # 2. URL_Length\n",
    "    attributes['URL_Length'] = 1 if len(url) < 54 else (0 if 54 <= len(url) <= 75 else -1)\n",
    "\n",
    "    # 3. Shortining_Service\n",
    "    attributes['Shortining_Service'] = -1 if \"tinyurl.com\" in url else 1\n",
    "\n",
    "    # 4. having_At_Symbol\n",
    "    attributes['Having_At_Symbol'] = -1 if '@' in parsed_url.netloc else 1\n",
    "\n",
    "    # 5. double_slash_redirecting\n",
    "    attributes['Double_Slash_Redirecting'] = -1 if '//' in url[7:] else 1\n",
    "\n",
    "    # 6. Prefix_Suffix\n",
    "    attributes['Prefix_Suffix'] = -1 if '-' in parsed_url.netloc else 1\n",
    "\n",
    "    # 7. having_Sub_Domain\n",
    "    attributes['Having_Sub_Domain'] = -1 if parsed_url.hostname.count('.') == 1 else (0 if parsed_url.hostname.count('.') == 0 else 1)\n",
    "\n",
    "    # 8. SSLfinal_State\n",
    "    attributes['SSLfinal_State'] = check_ssl_state(url)\n",
    "\n",
    "    # 9. Domain_registeration_length\n",
    "    attributes['Domain_Registeration_Length'] = check_domain_registration_length(url)\n",
    "\n",
    "    # 10. Favicon\n",
    "    attributes['Favicon'] = check_favicon(url)\n",
    "\n",
    "    # 11. Ports\n",
    "    attributes['Ports'] = check_ports(url)\n",
    "\n",
    "    # 12. HTTPS_token\n",
    "    subdomain = parsed_url.hostname.split('.')[0] if parsed_url.hostname else None\n",
    "    attributes['HTTPS_token'] = 1 if subdomain is None or 'https' not in subdomain else -1\n",
    "    \n",
    "    # 13. Request_URL\n",
    "    attributes['Request_URL'] = check_request_url(url)\n",
    "\n",
    "    # 14. URL_of_Anchor\n",
    "    attributes['URL_of_Anchor'] = check_url_of_anchor(url)\n",
    "\n",
    "    # 15. Links_in_tags\n",
    "    attributes['Links_in_tags'] = check_links_in_tags(url)\n",
    "\n",
    "    # 16. SFH\n",
    "    attributes['SFH'] = check_sfh(url)\n",
    "\n",
    "    # 17. Submitting_to_email\n",
    "    attributes['Submitting_To_Email'] = check_mail(url)\n",
    "\n",
    "    # 18. Abnormal_URL\n",
    "    attributes['Abnormal_URL'] = check_abnormal_url(url)\n",
    "\n",
    "    # 19. Redirect\n",
    "    attributes['Redirect'] = check_redirect(url)\n",
    "\n",
    "    # 20. on_mouseover\n",
    "    attributes['On_MouseOver'] = check_onMouseOver(url)\n",
    "\n",
    "    # 21. RightClick\n",
    "    attributes['RightClick'] = check_rightClick(url)\n",
    "\n",
    "    # 22. popUpWidnow\n",
    "    attributes['PopUpWidnow'] = check_popup(url)\n",
    "\n",
    "    # 23. Iframe\n",
    "    attributes['IFrame'] = check_iframe(url)\n",
    "\n",
    "    # 24. age_of_domain\n",
    "    attributes['Age_Of_Domain'] = check_age_of_domain(url)\n",
    "\n",
    "    # 25. DNSRecord\n",
    "    attributes['DNSRecord'] = check_dns_record(url)\n",
    "\n",
    "    # Attributes 26 & 27 are omitted, as they require APIs which are now deprecated.\n",
    "\n",
    "    # 28. Google_Index\n",
    "    attributes['Google_Index'] = check_google_index(url)\n",
    "\n",
    "    # 29. Links_pointing_to_page\n",
    "    attributes['Links_pointing_to_page'] = check_links_to_webpage(url)\n",
    "\n",
    "    # 30. Statistical_report\n",
    "    top_domains = [\"esy.es\", \"hol.es\", \"000webhostapp.com\", \"16mb.com\", \"bit.ly\", \"for-our.info\", \"beget.tech\", \"blogspot.com\", \"weebly.com\", \"raymannag.ch\"]\n",
    "    top_ips = [\"146.112.61.108\", \"31.170.160.61\", \"67.199.248.11\", \"67.199.248.10\", \"69.50.209.78\", \"192.254.172.78\", \"216.58.193.65\", \"23.234.229.68\", \"173.212.223.160\", \"60.249.179.122\"]\n",
    "    attributes['Statistical_report'] = compare_url_to_top_lists(url, top_domains, top_ips)\n",
    "\n",
    "    return attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Having_IP_Address': 1, 'URL_Length': 1, 'Shortining_Service': 1, 'Having_At_Symbol': 1, 'Double_Slash_Redirecting': 1, 'Prefix_Suffix': 1, 'Having_Sub_Domain': 1, 'SSLfinal_State': 1, 'Domain_Registeration_Length': -1, 'Favicon': 1, 'Ports': 1, 'HTTPS_token': 1, 'Request_URL': -1, 'URL_of_Anchor': 1, 'Links_in_tags': 1, 'SFH': -1, 'Submitting_To_Email': 1, 'Abnormal_URL': 1, 'Redirect': 1, 'On_MouseOver': 1, 'RightClick': 1, 'PopUpWidnow': 1, 'IFrame': -1, 'Age_Of_Domain': 1, 'DNSRecord': 1, 'Google_Index': 1, 'Links_pointing_to_page': 1, 'Statistical_report': 1}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.youtube.com\"\n",
    "attributes = process_url(url)\n",
    "print(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(file_path, attributes):\n",
    "    file_exists = True\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            pass\n",
    "    except FileNotFoundError:\n",
    "        file_exists = False\n",
    "\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        fieldnames = attributes.keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"processed_urls.csv\"\n",
    "append_to_csv(csv_file_path, attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
